{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563c84af",
   "metadata": {},
   "source": [
    "# Epsilon TSP Job Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c5b41c",
   "metadata": {},
   "source": [
    "The following notebook contains code and plots analysing Epsilons job configurations used for their **AggregateTSPMetricsJob-Resources** CDE job.\n",
    "\n",
    "Cloudera has analysed the following job configurations\n",
    "\n",
    "- 4 CPU Cores, 8GB Memory\n",
    "- 2 CPU Cores, 8GB Memory\n",
    "- 5 CPU Cores, 8GB Memory\n",
    "Each jobs analysis has been done with a minimum of 200 jobs that were run with those configurations.\n",
    "\n",
    "**The data for these jobs is fetched via the CDE API, as the logs for these jobs are also parsed to identify errors, the sample size has been kept small.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d22d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xmltodict\n",
    "# pip install requests-toolbelt\n",
    "# pip install yagmail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306af939",
   "metadata": {},
   "source": [
    "## Importing the required Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48101974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, json, requests\n",
    "import pandas as pd\n",
    "import python_cde.cdeconnection as cde\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import requests\n",
    "import subprocess as sp\n",
    "#import yagmail\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "from matplotlib.pyplot import figure\n",
    "import epsilon_analysis as ea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5736b145",
   "metadata": {},
   "source": [
    "## CDE API Token Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9011a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting variables for script\n",
    "JOBS_API_URL = \"https://gxjqflc9.cde-q8k5xhqz.cdp-pcm.cpx9-02nn.cloudera.site/dex/api/v1\"\n",
    "WORKLOAD_USER = \"csso_adh_bigdata_devs\" #\"cdpusername\"\n",
    "WORKLOAD_PASSWORD = \"password\" #\"cdppwd\"\n",
    "\n",
    "# Instantiate the Connection to CDE\n",
    "cde_connection = cde.CdeConnection(JOBS_API_URL, WORKLOAD_USER, WORKLOAD_PASSWORD)\n",
    "TOKEN = cde_connection.set_cde_token()\n",
    "\n",
    "#headers for API Call\n",
    "headers = {\n",
    "    'Authroization': f\"Bearer {TOKEN}\",\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a2b5f",
   "metadata": {},
   "source": [
    "## Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d3a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_runs = requests.get(JOBS_API_URL+\"/jobs\", headers=headers)\n",
    "# print(job_runs)\n",
    "# print(type(job_runs))\n",
    "# job_runs.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4903725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Filter not working as expected\n",
    "# output = sp.getoutput(f'curl -s -H \"Authorization: Bearer {TOKEN}\" -H \"Content-Type: application/json\" -X GET \"https://gxjqflc9.cde-q8k5xhqz.cdp-pcm.cpx9-02nn.cloudera.site/dex/api/v1/job-runs?offset=20200&limit=100\"')\n",
    "# convert_to_json = json.loads(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c29aa",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa9cdf6",
   "metadata": {},
   "source": [
    "The following functions have been defined\n",
    "\n",
    "- **get_data()**:- Takes a Job ID which will be our starting point along with a number which will be multiplied by 100 as the CDE API fetches job runs in batches of 100. This function returns a dataframe with the raw data collected from the API\n",
    "- **filter_data()**:- Takes raw API data and performs filtering(filters TSP jobs) and cleaning such as dropping NaNs, converting data types of columns to datatime and int where needed\n",
    "- **cores_memory()**:- Takes filterd jobs data and extracts the CPU core and Memory values from the \"spark\" object type column\n",
    "- **add_error_column()**:- Most of Epsilon jobs fail due to these errors\n",
    "  - No Eligible Deployments found from cmd view s3 files\n",
    "  - OOMKilled\n",
    "  - java.lang.String cannot be cast to java.sql.Timestamp\n",
    "This function creates a new column \"Error_Type\". It parses through the logs of each job searching for the above errors. When an error is found,it is added to the Error_Type column of the Job ID\n",
    "\n",
    "**add_runtimes()**:- This function calculates the runtimes of each jobs in minutes using the \"started\" and \"ended\" columns\n",
    "**waittime()**:- This function extracts the app_start_time and app_stop_time from each jobs log and calculates the wait time in minutes by subtracting the started and app_start_time values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25fdcfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the CDE Jobs API and storing the data into a data frame\n",
    "def get_data(offset_start,batches):\n",
    "    runs = list(range(offset_start, (batches*100)+ offset_start, 100))\n",
    "    raw_data = pd.DataFrame()\n",
    "    for offset in runs:\n",
    "        output = sp.getoutput(f'curl -s -H \"Authorization: Bearer {TOKEN}\" -H \"Content-Type: application/json\" -X GET \"https://gxjqflc9.cde-q8k5xhqz.cdp-pcm.cpx9-02nn.cloudera.site/dex/api/v1/job-runs?offset={offset}&limit=100\"')\n",
    "        convert_to_json = json.loads(output)\n",
    "        json_df = pd.DataFrame(convert_to_json[\"runs\"])\n",
    "        raw_data = raw_data.append(json_df, ignore_index=True)\n",
    "    return raw_data\n",
    "\n",
    "#Cleaning and Filtering\n",
    "def filter_data(raw_data):\n",
    "    raw_data = raw_data[['id','job', 'status', 'started', 'ended', 'spark']]\n",
    "    TSP_filter = raw_data[\"job\"] == \"AggregateTSPMetricsJob\" #Only TSP Jobs\n",
    "    raw_data = raw_data.where(TSP_filter)\n",
    "    issues = raw_data[raw_data[\"spark\"] == {}].index\n",
    "    raw_data = raw_data.drop(issues)\n",
    "    raw_data = raw_data.dropna() # Drop any NA\n",
    "    raw_data[\"status\"] = raw_data[\"status\"].astype('category')\n",
    "    raw_data[\"id\"] = raw_data[\"id\"].astype('Int64') # Convert \"Id\" to Int\n",
    "    raw_data[\"started\"] = pd.to_datetime(raw_data[\"started\"]) #Convert \"started\" to DateTime\n",
    "    raw_data[\"ended\"] = pd.to_datetime(raw_data[\"ended\"]) #Convert \"ended\" to DateTime\n",
    "    raw_data = raw_data.reset_index() # Reset dataframe index\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c58e6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cores and memory of the job from \"spark\" complex object\n",
    "def cores_memory(clean_data):\n",
    "    cores_list = [] \n",
    "    memory_list = []\n",
    "    org_list = []\n",
    "    init_exec = []\n",
    "   \n",
    "\n",
    "    for id in clean_data.index:\n",
    "        cores_list.append(clean_data.spark[id][\"spec\"][\"executorCores\"])\n",
    "    for id in clean_data.index:\n",
    "        memory_list.append(clean_data.spark[id][\"spec\"][\"executorMemory\"])\n",
    "    for id in clean_data.index:\n",
    "        if \"spark.dynamicAllocation.initialExecutors\" in clean_data.spark[id][\"spec\"][\"conf\"]:\n",
    "            init_exec.append(clean_data.spark[id][\"spec\"][\"conf\"][\"spark.dynamicAllocation.initialExecutors\"])\n",
    "        else:\n",
    "            init_exec.append(15)\n",
    "    for id in clean_data.index:\n",
    "        result = clean_data[\"spark\"][id][\"spec\"][\"args\"][0]\n",
    "        result_to_json = json.loads(result)\n",
    "        result_json_df = pd.DataFrame(result_to_json)\n",
    "        org_list.append(result_json_df[\"orgId\"][0])\n",
    "        \n",
    "\n",
    "    cores_series = pd.Series(cores_list)\n",
    "    memory_series = pd.Series(memory_list)\n",
    "    org_series = pd.Series(org_list)\n",
    "    init_exec_series = pd.Series(init_exec)\n",
    "\n",
    "    # Create new dataframe with reduced columns\n",
    "    inter_df = clean_data[['id','job', 'status', 'started', 'ended']]\n",
    "\n",
    "    #Add Cores and Memory columns to dataframe\n",
    "    inter_df[\"cores\"] = cores_series.values\n",
    "    inter_df[\"memory\"] = memory_series.values\n",
    "    inter_df[\"orgId\"] = org_series.values\n",
    "    inter_df[\"orgId\"] = inter_df[\"orgId\"].astype('category')\n",
    "    inter_df[\"init_exec\"] = init_exec_series.values\n",
    "    inter_df[\"init_exec\"] = inter_df[\"init_exec\"].astype(\"category\")\n",
    "    return inter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9834927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe containing all the errors\n",
    "def add_error_column(inter_df):\n",
    "    s3_error = []\n",
    "    oom_error = []\n",
    "    cast_error = []\n",
    "    success = []\n",
    "    misc = []\n",
    "    \n",
    "    success_df = inter_df[inter_df.status == \"succeeded\"]\n",
    "    for job_run_id in success_df[\"id\"]:\n",
    "        success.append(job_run_id)\n",
    "    \n",
    "    fail_df = inter_df[inter_df.status == \"failed\"]    \n",
    "    for job_run_id in fail_df[\"id\"]:\n",
    "        logs = sp.getoutput(f'curl -s -H \"Authorization: Bearer {TOKEN}\" -H \"Content-Type: application/json\" -X GET \"https://gxjqflc9.cde-q8k5xhqz.cdp-pcm.cpx9-02nn.cloudera.site/dex/api/v1/job-runs/{job_run_id}/logs?type=driver/stderr&tailLines=10000\"')\n",
    "        if \"No Eligible Deployments found from cmd view s3 files\" in logs:\n",
    "            s3_error.append(job_run_id)\n",
    "        elif \"OOMKilled\" in logs:\n",
    "            oom_error.append(job_run_id)\n",
    "        elif \"java.lang.String cannot be cast to java.sql.Timestamp\" in logs:\n",
    "            cast_error.append(job_run_id)\n",
    "        else:\n",
    "            misc.append(job_run_id)\n",
    "    \n",
    "    \n",
    "    all_errors = [s3_error, oom_error, cast_error, misc, success]\n",
    "    errors_df = pd.DataFrame((_ for _ in itertools.zip_longest(*all_errors)), columns=[\"S3_Error\", \"OOM_Error\", \"Cast_Error\", \"Misc\", \"Success\"])\n",
    "    errors_df[\"OOM_Error\"] = errors_df[\"OOM_Error\"].astype('category')\n",
    "    errors_df[\"Cast_Error\"] = errors_df[\"Cast_Error\"].astype('category')\n",
    "    errors_df[\"Misc\"] = errors_df[\"Misc\"].astype('category')\n",
    "    errors_df[\"Success\"] = errors_df[\"Success\"].astype('category')\n",
    "\n",
    "    # Add \"Error_Type\" column in dataframe\n",
    "    inter_df[\"Error_Type\"] = np.nan\n",
    "\n",
    "    # Loop through each id in the dataset and assign it an Error_Type\n",
    "    for i in inter_df[\"id\"]:\n",
    "        if i in errors_df[\"S3_Error\"].values:\n",
    "            index = inter_df.index[inter_df[\"id\"] == i].tolist()\n",
    "            inter_df[\"Error_Type\"][index] = \"S3_Error\"\n",
    "        elif i in errors_df[\"OOM_Error\"].values:\n",
    "            index = inter_df.index[inter_df[\"id\"] == i].tolist()\n",
    "            inter_df[\"Error_Type\"][index] = \"OOM_Error\"\n",
    "        elif i in errors_df[\"Cast_Error\"].values:\n",
    "            index = inter_df.index[inter_df[\"id\"] == i].tolist()\n",
    "            inter_df[\"Error_Type\"][index] = \"Cast_Error\"\n",
    "        elif i in errors_df[\"Success\"].values:\n",
    "            index = inter_df.index[inter_df[\"id\"] == i].tolist()\n",
    "            inter_df[\"Error_Type\"][index] = \"Successful Job\"\n",
    "        else:\n",
    "            index = inter_df.index[inter_df[\"id\"] == i].tolist()\n",
    "            inter_df[\"Error_Type\"][index] = \"Misc\"\n",
    "    #Set \"Error_Type\" column as categorical variable\n",
    "    inter_df[\"Error_Type\"] = inter_df[\"Error_Type\"].astype('category')\n",
    "    print(f'Jobs with S3 error:{s3_error}')\n",
    "    print(f'Total: {len(s3_error)}')\n",
    "    print(f'Jobs with OOM error:{oom_error}')\n",
    "    print(f'Total: {len(oom_error)}')\n",
    "    print(f'Jobs with Casting error:{cast_error}')\n",
    "    print(f'Total: {len(cast_error)}')\n",
    "    print(f'Misc - Success :{success}')\n",
    "    print(f'Total: {len(success)}')\n",
    "    print(f'Misc - Killed/Failed Due to other reasons :{misc}')\n",
    "    print(f'Total: {len(misc)}')\n",
    "    \n",
    "    return inter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b27bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_runtimes(inter_df):\n",
    "    #Add \"runtime\" variable by subtracting \"ended\" and \"started\"\n",
    "\n",
    "    inter_df[\"runtime\"] = inter_df[\"ended\"] - inter_df[\"started\"]\n",
    "    inter_df.head()\n",
    "\n",
    "    # Set \"runtime\" column to minutes\n",
    "    inter_df[\"runtime\"] = inter_df[\"runtime\"] / pd.Timedelta(minutes=1)\n",
    "    inter_df[\"runtime\"]\n",
    "    \n",
    "    return inter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f771122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def waittime(inter_df):\n",
    "    start_time = []\n",
    "    stop_time = []\n",
    "    job_id = []\n",
    "    for job_run_id in inter_df[\"id\"]:\n",
    "        logs = sp.getoutput(f'curl -s -H \"Authorization: Bearer {TOKEN}\" -H \"Content-Type: application/json\" -X GET \"https://gxjqflc9.cde-q8k5xhqz.cdp-pcm.cpx9-02nn.cloudera.site/dex/api/v1/job-runs/{job_run_id}/logs?type=driver/event\"')\n",
    "        words = logs.splitlines()\n",
    "        try:\n",
    "            for word in words:\n",
    "                event = json.loads(word)\n",
    "                if event[\"Event\"] == \"SparkListenerApplicationStart\":\n",
    "                    timestamp = event[\"Timestamp\"]/1000\n",
    "                    start_time_convert = datetime.datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    start_time.append(start_time_convert)\n",
    "        except Exception: \n",
    "            print(f\"App Start Time JSONDecodeError {job_run_id}\")\n",
    "        try:\n",
    "            for word in words:\n",
    "                event = json.loads(word)\n",
    "                if event[\"Event\"] == \"SparkListenerApplicationEnd\":\n",
    "                    timestamp = event[\"Timestamp\"]/1000\n",
    "                    stop_time_convert = datetime.datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    stop_time.append(stop_time_convert)\n",
    "        except Exception: \n",
    "            print(f\"App Stop Time JSONDecodeError {job_run_id}\")\n",
    "\n",
    "        job_id.append(job_run_id)\n",
    "    \n",
    "    start_series = pd.Series(start_time)\n",
    "    stop_series = pd.Series(stop_time)\n",
    "    inter_df[\"app_start_time\"] = pd.Series(start_series)\n",
    "    inter_df[\"app_stop_time\"] = pd.Series(stop_series)\n",
    "    inter_df[\"app_start_time\"] = pd.to_datetime(inter_df[\"app_start_time\"], utc=True)\n",
    "    inter_df[\"app_stop_time\"] = pd.to_datetime(inter_df[\"app_stop_time\"], utc=True)\n",
    "    inter_df[\"waitTime\"] = inter_df[\"app_start_time\"] - inter_df[\"started\"]\n",
    "    inter_df[\"waitTime\"] = inter_df[\"waitTime\"] / pd.Timedelta(minutes=1)\n",
    "    print(f'Total: {len(start_time)}')\n",
    "    print(f'Total: {len(stop_time)}')\n",
    "#     print(f'Job IDs: {job_id}')\n",
    "#     print(f'Total Job IDs: {len(job_id)}')\n",
    "    return inter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f03e055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6181f82f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "479767bb",
   "metadata": {},
   "source": [
    "## Analysis for 6vCPU 24GB with 1 Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "732d4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_start = 34676 #Jobs will be retrieved starting from this Job Run ID\n",
    "batches = 10 # Number*100 of jobs will be fetched, e.g. When value is 4 -> 4*100 = 400 jobs Original 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dba575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs with S3 error:[34679, 34681, 34683, 34686, 34687, 34688, 34690, 34692, 34695, 34697, 34700, 34703, 34704, 34705, 34708, 34711, 34713, 34715, 34717, 34718, 34719, 34720, 34721, 34722, 34724, 34726, 34729, 34731, 34734, 34737, 34738, 34739, 34740, 34743, 34744, 34747, 34749, 34752, 34755, 34756, 34757, 34758, 34761, 34764, 34766, 34769, 34771, 34772, 34773, 34774, 34776, 34778, 34781, 34784, 34786, 34787, 34788, 34789, 34790, 34791, 34794, 34797, 34799, 34801, 34803, 34804, 34805, 34806, 34808, 34815, 34816, 34818, 34820, 34822, 34823, 34824, 34825, 34826, 34827, 34831, 34834, 34836, 34839, 34841, 34842, 34843, 34844, 34847, 34848, 34851, 34854, 34856, 34857, 34858, 34859, 34860, 34861, 34862, 34865, 34868, 34870, 34873, 34874, 34875, 34876, 34877, 34885, 34887, 34890, 34892, 34895, 34898, 34899, 34900, 34901, 34904, 34905, 34908, 34910, 34913, 34915, 34916, 34917, 34918, 34921, 34922, 34924, 34926, 34928, 34930, 34931, 34932, 34933, 34936, 34937, 34940, 34942, 34945, 34948, 34949, 34955, 34957, 34959, 34975, 34976, 34978, 34980, 34982, 34983, 34984, 34987, 34989, 34991, 34994, 34996, 34999, 35001, 35002, 35003, 35004, 35008, 35010, 35013, 35015, 35018, 35019, 35024, 35025, 35028, 35030, 35033, 35036, 35037, 35038, 35039, 35042, 35044, 35046, 35049, 35052, 35063, 35066, 35068, 35071, 35091, 35093, 35094, 35096, 35100, 35101, 35102, 35103, 35106, 35107, 35108, 35109, 35113, 35120, 35123, 35125, 35128, 35131, 35143, 35144, 35147, 35149, 35151, 35160, 35164, 35165, 35169, 35188, 35189, 35191, 35192, 35194, 35196, 35197, 35199, 35200, 35202, 35203, 35205, 35206, 35209, 35210, 35211, 35212, 35213, 35214, 35219, 35226, 35227, 35228, 35229, 35230, 35232, 35233, 35235, 35236, 35239, 35240, 35242, 35243, 35245, 35246, 35247, 35249, 35259, 35263, 35265, 35268, 35271, 35289, 35290, 35292, 35293, 35295, 35301, 35303, 35304, 35305, 35306, 35307, 35308, 35309, 35311, 35312, 35314, 35317, 35324, 35327, 35330, 35332, 35335, 35348, 35350, 35351, 35354, 35356, 35362, 35365, 35369, 35371, 35374, 35399, 35400, 35402, 35403, 35405, 35420, 35421, 35423, 35425, 35426, 35428, 35429, 35431, 35432, 35435, 35436, 35437, 35439, 35443, 35445, 35446, 35447, 35449, 35450, 35451, 35452, 35475, 35477, 35480, 35482, 35486, 35495, 35498, 35502, 35504, 35523, 35525, 35527, 35529, 35534, 35535, 35537, 35538, 35539, 35541, 35542, 35543, 35545, 35546, 35547, 35548, 35549, 35550, 35553, 35556, 35559, 35560, 35563, 35566, 35569, 35577, 35579, 35583, 35586, 35589, 35605, 35606, 35608, 35610, 35612, 35629, 35630, 35634, 35637, 35640, 35651, 35653, 35657, 35659, 35672, 35675]\n",
      "Total: 366\n",
      "Jobs with OOM error:[]\n",
      "Total: 0\n",
      "Jobs with Casting error:[35110, 35111, 35216, 35217, 35218, 35220, 35221, 35251, 35252, 35253, 35313, 35315, 35316, 35440, 35442, 35453, 35454, 35551, 35552, 35554, 35555]\n",
      "Total: 21\n",
      "Misc - Success :[34830, 34954, 35007, 35062, 35090, 35105, 35158, 35493, 35522, 35648]\n",
      "Total: 10\n",
      "Misc - Killed/Failed Due to other reasons :[]\n",
      "Total: 0\n",
      "CPU times: total: 5.36 s\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Creating new dataframe for Clean Data\n",
    "raw_data = get_data(offset_start, batches)\n",
    "clean_data = filter_data(raw_data)\n",
    "clean_data = cores_memory(clean_data)\n",
    "clean_data = add_error_column(clean_data)\n",
    "clean_data = add_runtimes(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81a0afef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 397\n",
      "Total: 397\n"
     ]
    }
   ],
   "source": [
    "clean_data = waittime(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8cd4c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df24_exec_1 = clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7662fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df24_exec_1.to_csv(\"six_core_24_exec_1_Jan19.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c573b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cb8d2bd",
   "metadata": {},
   "source": [
    "## Analysis for 6vCPU 24GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c84f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_start = 21802 #Jobs will be retrieved starting from this Job Run ID\n",
    "batches = 12 # Number*100 of jobs will be fetched, e.g. When value is 4 -> 4*100 = 400 jobs Original 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dda8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Creating new dataframe for Clean Data\n",
    "raw_data = get_data(offset_start, batches)\n",
    "clean_data = filter_data(raw_data)\n",
    "clean_data = cores_memory(clean_data)\n",
    "clean_data = add_error_column(clean_data)\n",
    "clean_data = add_runtimes(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a95561e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_data = waittime(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df24 = clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb6b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df24.to_csv(\"six_core_24.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3331c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ded0fc75",
   "metadata": {},
   "source": [
    "## Analysis for 6vCPU 14GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd92fd",
   "metadata": {},
   "source": [
    "Job ID 21174 was the first job run with the configuration of 6 cores and 14GB of memory. It was run on Dec 21st 06:00 PM. We have used a batch size of 4, which will fetch 400 jobs for us, these will be filtered and reduced down further to create our sample data of **354 TSP Jobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa86bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_start = 21174 #Jobs will be retrieved starting from this Job Run ID\n",
    "batches = 6 # Number*100 of jobs will be fetched, e.g. When value is 4 -> 4*100 = 400 jobs Original 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab3ea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Creating new dataframe for Clean Data\n",
    "raw_data = get_data(offset_start, batches)\n",
    "clean_data = filter_data(raw_data)\n",
    "clean_data = cores_memory(clean_data)\n",
    "clean_data = add_error_column(clean_data)\n",
    "clean_data = add_runtimes(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57e5fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clean_data = waittime(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccdd383",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df14 = clean_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d02200",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df14.to_csv(\"six_core_14.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1714275",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df14.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d7f6c",
   "metadata": {},
   "source": [
    "# Analysis for 4vCPU 8GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a7ed1",
   "metadata": {},
   "source": [
    "Job ID 20243 was the first job run with the configuration of 4 cores and 8GB of memory. It was run on We have used a batch size of 4, which will fetch 400 jobs for us, these will be filtered and reduced down further to create our sample data of **354 TSP Jobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_start = 20242 #Jobs will be retrieved starting from this Job Run ID\n",
    "batches = 4 # Number*100 of jobs will be fetched, e.g. When value is 4 -> 4*100 = 400 jobs Original 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Creating new dataframe for Clean Data\n",
    "raw_data = get_data(offset_start, batches)\n",
    "clean_data = filter_data(raw_data)\n",
    "clean_data = cores_memory(clean_data)\n",
    "clean_data = add_error_column(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984fbded",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = add_runtimes(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50fe969",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clean_data = waittime(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: investigate Run ID 20301. It succeeded, but had an OOM error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b55456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df4 = clean_data\n",
    "final_df4.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49284553",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df4.to_csv(\"four_core_8.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07284748",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e787e64",
   "metadata": {},
   "source": [
    "The following visualization shows us the runtimes of each job with 4 cores and 8GB memory configured. **The average run time of hese jobs ~40 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f3534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52efdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs = final_df4\n",
    "jobs_by_org = jobs.groupby(['orgId']).count()\n",
    "jobs_by_org = jobs_by_org[\"id\"]\n",
    "jobs_by_org.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a568e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = final_df4\n",
    "jobs_by_org = jobs.groupby(['orgId',\"status\"]).count()\n",
    "jobs_by_org = jobs_by_org[\"id\"]\n",
    "jobs_by_org.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9a2716",
   "metadata": {},
   "source": [
    "## Out of Memory Error by Organisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2067bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "oom_error = final_df4[final_df4[\"Error_Type\"] == \"OOM_Error\"]\n",
    "error_by_org = oom_error.groupby(['orgId','Error_Type']).count()\n",
    "error_by_org = error_by_org[\"id\"]\n",
    "error_by_org.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455e7ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_by_org.tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "ax = error_by_org.unstack(level=1).plot(kind='bar', subplots=True, rot=0, figsize=(16,10), layout=(4,1), xlabel = \"Status\", ylabel = \"Count\", title = \"Individual Cores Distribution\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8e81c",
   "metadata": {},
   "source": [
    "## ScatterPlot of Job Runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed42adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get required data\n",
    "x = final_df4[\"id\"]\n",
    "y = final_df4[\"runtime\"]\n",
    "\n",
    "#Get unique status\n",
    "uniq = list(set(final_df4[\"status\"]))\n",
    "\n",
    "# Set color map to match number of status types\n",
    "z = range(1, len(uniq))\n",
    "hot = plt.get_cmap('hot')\n",
    "cnorm = colors.Normalize(vmin=0, vmax=len(uniq))\n",
    "scalarMap = cmx.ScalarMappable(norm=cnorm, cmap=hot)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "# Plot each status\n",
    "for i in range(len(uniq)):\n",
    "    indx = final_df4[\"status\"] == uniq[i]\n",
    "    plt.scatter(x[indx], y[indx], s = 50, color=scalarMap.to_rgba(i), label=uniq[i])\n",
    "\n",
    "#Calculate Mean Run time\n",
    "runtime_mean = [np.mean(final_df4[\"runtime\"])]*len(final_df4.index)\n",
    "print(f\"Average Runtime:{runtime_mean[0]}\")\n",
    "\n",
    "\n",
    "#Add Plot details\n",
    "plt.title(\"Runtime of Jobs\", size = 15)\n",
    "plt.xlabel(\"Job ID\", size = 15)\n",
    "plt.ylabel(\"Minutes\", size = 15)\n",
    "plt.legend(loc='upper right', fontsize = 15)\n",
    "#Add Mean Line\n",
    "plt.plot(final_df4[\"id\"],runtime_mean, label='Mean', linestyle='-', color = 'blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36350b1",
   "metadata": {},
   "source": [
    "We can see from the below bar plot that out 354 jobs runs:\n",
    "\n",
    "- **167 failed with an S3 error**\n",
    "- **120 failed with an Out of Memory Error**\n",
    "- **55 failed with a casting error**\n",
    "\n",
    "and the rest were Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4b710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error Distribution\n",
    "sns.set(rc={'figure.figsize':(15,7)})\n",
    "ax = sns.countplot(final_df4[\"Error_Type\"], order = final_df4[\"Error_Type\"].value_counts().index)\n",
    "ax.set_title(\"Error Type Distribution\", size = 15)\n",
    "ax.set_xlabel(\"Error_Type\", size=15)\n",
    "ax.set_ylabel(\"Count\", size=15)\n",
    "ax.bar_label(ax.containers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751fdf1b",
   "metadata": {},
   "source": [
    "# Analysis for 2 vCPU 8GB Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f86cc5f",
   "metadata": {},
   "source": [
    "Job ID 12916 was run on December 02nd with 2 cores and 8GB of memory. We will be using a batch size of 8 to collect a sufficient sample size. **The Sample size for this configuration is 212**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50530623",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_start = 12916 #Jobs will be retrieved starting from this Job Run ID\n",
    "batches = 8 # Number*100 of jobs will be fetched, e.g. When value is 4 -> 4*100 = 400 jobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08df6a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new dataframe for Clean Data\n",
    "raw_data = get_data(offset_start, batches)\n",
    "clean_data = filter_data(raw_data)\n",
    "clean_data = cores_memory(clean_data)\n",
    "clean_data = add_runtimes(clean_data)\n",
    "clean_data = add_error_column(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clean_data = waittime(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9454c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df2 = clean_data\n",
    "final_df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df2.to_csv(\"two_core_8.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e27ba67",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b26c7a",
   "metadata": {},
   "source": [
    "## Out of Memory Error by Organisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8205e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "oom_error = final_df2[final_df2[\"Error_Type\"] == \"OOM_Error\"]\n",
    "error_by_org = oom_error.groupby(['orgId','Error_Type']).count()\n",
    "error_by_org = error_by_org[\"id\"]\n",
    "error_by_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e0ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "ax = error_by_org.unstack(level=1).plot(kind='bar', subplots=True, rot=0, figsize=(16,10), layout=(4,1), xlabel = \"Status\", ylabel = \"Count\", title = \"Individual Cores Distribution\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f10a9e",
   "metadata": {},
   "source": [
    "## ScatterPlot of Job Runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764bd11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get required data\n",
    "x = final_df2[\"id\"]\n",
    "y = final_df2[\"runtime\"]\n",
    "\n",
    "#Get unique status\n",
    "uniq = list(set(final_df2[\"status\"]))\n",
    "\n",
    "# Set color map to match number of status types\n",
    "z = range(1, len(uniq))\n",
    "hot = plt.get_cmap('hot')\n",
    "cnorm = colors.Normalize(vmin=0, vmax=len(uniq))\n",
    "scalarMap = cmx.ScalarMappable(norm=cnorm, cmap=hot)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "# Plot each status\n",
    "for i in range(len(uniq)):\n",
    "    indx = final_df2[\"status\"] == uniq[i]\n",
    "    plt.scatter(x[indx], y[indx], s = 50, color=scalarMap.to_rgba(i), label=uniq[i])\n",
    "\n",
    "#Calculate Mean Run time\n",
    "runtime_mean = [np.mean(final_df2[\"runtime\"])]*len(final_df2.index)\n",
    "print(f\"Average Runtime:{runtime_mean[0]}\")\n",
    "\n",
    "\n",
    "#Add Plot details\n",
    "plt.title(\"Runtime of Jobs\", size = 15)\n",
    "plt.xlabel(\"Job ID\", size = 15)\n",
    "plt.ylabel(\"Minutes\", size = 15)\n",
    "plt.legend(loc='upper right', fontsize = 15)\n",
    "#Add Mean Line\n",
    "plt.plot(final_df2[\"id\"],runtime_mean, label='Mean', linestyle='-', color = 'blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cdc679",
   "metadata": {},
   "source": [
    "We can see from the below bar plot that out 212 jobs runs:\n",
    "\n",
    "- **162 failed with an S3 error**\n",
    "- **31 failed with a casting error**\n",
    "- **12 failed with an Out of Memory Error**\n",
    "\n",
    "and the rest were Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dafa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error Distribution\n",
    "sns.set(rc={'figure.figsize':(15,7)})\n",
    "ax = sns.countplot(final_df2[\"Error_Type\"], order = final_df2[\"Error_Type\"].value_counts().index)\n",
    "ax.set_title(\"Error Type Distribution\", size = 15)\n",
    "ax.set_xlabel(\"Error_Type\", size=15)\n",
    "ax.set_ylabel(\"Count\", size=15)\n",
    "ax.bar_label(ax.containers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db690646",
   "metadata": {},
   "source": [
    "# Analysis for 5 vCPU and 8 GB Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56664d43",
   "metadata": {},
   "source": [
    "Job ID 10678 was run on November 24th with 5 cores and 8GB of memory. We will be using a batch size of 12 to collect a sufficient sample size. **The Sample size for this configuration is 232**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf9c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_start = 10678 #Jobs will be retrieved starting from this Job Run ID\n",
    "batches = 12 # Number*100 of jobs will be fetched, e.g. When value is 12 -> 12*100 = 1200 jobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new dataframe for Clean Data\n",
    "raw_data = get_data(offset_start, batches)\n",
    "clean_data = filter_data(raw_data)\n",
    "clean_data = cores_memory(clean_data)\n",
    "clean_data = add_runtimes(clean_data)\n",
    "clean_data = add_error_column(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78b604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = waittime(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6acec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.tail(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df5 = clean_data\n",
    "final_df5.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ace8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df5.to_csv(\"five_core_8.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a14c3a",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab60485",
   "metadata": {},
   "source": [
    "The following visualization shows us the runtimes of each job with 5 cores and 8GB memory configured. **The average run time of these jobs ~15 minutes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5957a5",
   "metadata": {},
   "source": [
    "## Out of Memory Error by Organisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317bc8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "oom_error = final_df5[final_df5[\"Error_Type\"] == \"OOM_Error\"]\n",
    "error_by_org = oom_error.groupby(['orgId','Error_Type']).count()\n",
    "error_by_org = error_by_org[\"id\"]\n",
    "error_by_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11082aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "ax = error_by_org.unstack(level=1).plot(kind='bar', subplots=True, rot=0, figsize=(16,10), layout=(4,1), xlabel = \"Status\", ylabel = \"Count\", title = \"Individual Cores Distribution\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156bf386",
   "metadata": {},
   "source": [
    "## ScatterPlot of Job Runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98cdfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get required data\n",
    "x = final_df5[\"id\"]\n",
    "y = final_df5[\"runtime\"]\n",
    "\n",
    "#Get unique status\n",
    "uniq = list(set(final_df5[\"status\"]))\n",
    "\n",
    "# Set color map to match number of status types\n",
    "z = range(1, len(uniq))\n",
    "hot = plt.get_cmap('hot')\n",
    "cnorm = colors.Normalize(vmin=0, vmax=len(uniq))\n",
    "scalarMap = cmx.ScalarMappable(norm=cnorm, cmap=hot)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "# Plot each status\n",
    "for i in range(len(uniq)):\n",
    "    indx = final_df5[\"status\"] == uniq[i]\n",
    "    plt.scatter(x[indx], y[indx], s = 50, color=scalarMap.to_rgba(i), label=uniq[i])\n",
    "\n",
    "#Calculate Mean Run time\n",
    "runtime_mean = [np.mean(final_df5[\"runtime\"])]*len(final_df5.index)\n",
    "print(f\"Average Runtime:{runtime_mean[0]}\")\n",
    "\n",
    "\n",
    "#Add Plot details\n",
    "plt.title(\"Runtime of Jobs\", size = 15)\n",
    "plt.xlabel(\"Job ID\", size = 15)\n",
    "plt.ylabel(\"Minutes\", size = 15)\n",
    "plt.legend(loc='upper right', fontsize = 15)\n",
    "#Add Mean Line\n",
    "plt.plot(final_df5[\"id\"],runtime_mean, label='Mean', linestyle='-', color = 'blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5404147",
   "metadata": {},
   "source": [
    "We can see from the below bar plot that out 212 jobs runs:\n",
    "\n",
    "- **117 failed with an Out of Memory Error**\n",
    "- **102 failed with an S3 error**\n",
    "\n",
    "and the rest were Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45783771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error Distribution\n",
    "sns.set(rc={'figure.figsize':(15,7)})\n",
    "ax = sns.countplot(final_df5[\"Error_Type\"], order = final_df5[\"Error_Type\"].value_counts().index)\n",
    "ax.set_title(\"Error Type Distribution\", size = 15)\n",
    "ax.set_xlabel(\"Error_Type\", size=15)\n",
    "ax.set_ylabel(\"Count\", size=15)\n",
    "ax.bar_label(ax.containers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db8892",
   "metadata": {},
   "source": [
    "# All Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7627753d",
   "metadata": {},
   "source": [
    "This section will be used to analyze all the jobs that have been run on the PCM Prod cluster. We will not be doing an error type analysis as parsing through logs of over 20,000 jobs will not be reasonable. This data(upto Job ID 20500) will be used for additional visualizations.\n",
    "\n",
    "**Close to ~7500 TSP jobs have been run on the PCM Prod Cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dad4d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0998d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_start = 10 #Jobs will be retrieved starting from this Job Run ID\n",
    "batches = 363 # Number*100 of jobs will be fetched, e.g. When value is 4 -> 4*100 = 400 jobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4fc608b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 36.9 s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Creating new dataframe for Clean Data\n",
    "raw_data = get_data(offset_start, batches)\n",
    "clean_data = filter_data(raw_data)\n",
    "clean_data = cores_memory(clean_data)\n",
    "clean_data = add_runtimes(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2ae5a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jobs = clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb19acaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job</th>\n",
       "      <th>status</th>\n",
       "      <th>started</th>\n",
       "      <th>ended</th>\n",
       "      <th>cores</th>\n",
       "      <th>memory</th>\n",
       "      <th>orgId</th>\n",
       "      <th>init_exec</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13553</th>\n",
       "      <td>36283</td>\n",
       "      <td>AggregateTSPMetricsJob</td>\n",
       "      <td>failed</td>\n",
       "      <td>2023-01-18 12:30:01+00:00</td>\n",
       "      <td>2023-01-18 12:34:34+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>24g</td>\n",
       "      <td>TRAINING_HDS_APAC_DEMO_BU_HDS</td>\n",
       "      <td>1</td>\n",
       "      <td>4.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13554</th>\n",
       "      <td>36285</td>\n",
       "      <td>AggregateTSPMetricsJob</td>\n",
       "      <td>failed</td>\n",
       "      <td>2023-01-18 12:36:02+00:00</td>\n",
       "      <td>2023-01-18 12:37:04+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>24g</td>\n",
       "      <td>P2DEMXLOYALTYBU_P2DEMX_Loyalty_BU_1559207031463</td>\n",
       "      <td>1</td>\n",
       "      <td>1.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13555</th>\n",
       "      <td>36288</td>\n",
       "      <td>AggregateTSPMetricsJob</td>\n",
       "      <td>failed</td>\n",
       "      <td>2023-01-18 12:42:03+00:00</td>\n",
       "      <td>2023-01-18 12:45:39+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>24g</td>\n",
       "      <td>TRAINING_HDS_HNY_ENV1_HDS</td>\n",
       "      <td>1</td>\n",
       "      <td>3.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13556</th>\n",
       "      <td>36291</td>\n",
       "      <td>AggregateTSPMetricsJob</td>\n",
       "      <td>failed</td>\n",
       "      <td>2023-01-18 12:48:05+00:00</td>\n",
       "      <td>2023-01-18 12:49:09+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>24g</td>\n",
       "      <td>EPS_SALES_HDS_DIG_INCORP_HDS</td>\n",
       "      <td>1</td>\n",
       "      <td>1.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13557</th>\n",
       "      <td>36295</td>\n",
       "      <td>AggregateTSPMetricsJob</td>\n",
       "      <td>failed</td>\n",
       "      <td>2023-01-18 12:54:08+00:00</td>\n",
       "      <td>2023-01-18 12:58:29+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>24g</td>\n",
       "      <td>CDP_AUTO_QEPROD_1_CDP_SUB_AUTO_QEPROD_1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.350000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                     job  status                   started  \\\n",
       "13553  36283  AggregateTSPMetricsJob  failed 2023-01-18 12:30:01+00:00   \n",
       "13554  36285  AggregateTSPMetricsJob  failed 2023-01-18 12:36:02+00:00   \n",
       "13555  36288  AggregateTSPMetricsJob  failed 2023-01-18 12:42:03+00:00   \n",
       "13556  36291  AggregateTSPMetricsJob  failed 2023-01-18 12:48:05+00:00   \n",
       "13557  36295  AggregateTSPMetricsJob  failed 2023-01-18 12:54:08+00:00   \n",
       "\n",
       "                          ended  cores memory  \\\n",
       "13553 2023-01-18 12:34:34+00:00      6    24g   \n",
       "13554 2023-01-18 12:37:04+00:00      6    24g   \n",
       "13555 2023-01-18 12:45:39+00:00      6    24g   \n",
       "13556 2023-01-18 12:49:09+00:00      6    24g   \n",
       "13557 2023-01-18 12:58:29+00:00      6    24g   \n",
       "\n",
       "                                                 orgId init_exec   runtime  \n",
       "13553                    TRAINING_HDS_APAC_DEMO_BU_HDS         1  4.550000  \n",
       "13554  P2DEMXLOYALTYBU_P2DEMX_Loyalty_BU_1559207031463         1  1.033333  \n",
       "13555                        TRAINING_HDS_HNY_ENV1_HDS         1  3.600000  \n",
       "13556                     EPS_SALES_HDS_DIG_INCORP_HDS         1  1.066667  \n",
       "13557          CDP_AUTO_QEPROD_1_CDP_SUB_AUTO_QEPROD_1         1  4.350000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_jobs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "492a5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jobs.to_csv(\"All_Jobs_Data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "639261fb",
   "metadata": {},
   "source": [
    "The below pie chart shows that 96% of TSP jobs run on the cluster have failed. **Only 286 of the 7486 jobs run on the cluster have succeeded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e6fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autopct_format(values):\n",
    "        def my_format(pct):\n",
    "            total = sum(values)\n",
    "            val = int(round(pct*total/100.0))\n",
    "            return '{:.1f}%\\n({v:d})'.format(pct, v=val)\n",
    "        return my_format\n",
    "\n",
    "\n",
    "s = all_jobs['status'].value_counts()\n",
    "explode = (0,0.7,0.5)\n",
    "plt.pie(s,labels = s.index, explode = explode, autopct=autopct_format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa28b692",
   "metadata": {},
   "source": [
    "## Breakdown of Jobs based on Cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b322c9",
   "metadata": {},
   "source": [
    "Break down of Epsilon job runs on the basis of CPU cores\n",
    "\n",
    "**5 Cores** | **4 Cores** | **2 Cores**\n",
    "--- | --- | ---\n",
    "Failed: 4157 | Failed: 315 | Failed: 2716\n",
    "Killed: 11 | Killed: NA | Killed: 2\n",
    "Succeeded: 219 | Succeeded: 13 | Succeeded: 54\n",
    "**Total: 4387** | **Total: 328** | **Total: 2772** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ff5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_distr = all_jobs.groupby(['status','cores']).count()\n",
    "job_distr = job_distr[\"id\"]\n",
    "print(job_distr)\n",
    "ax = job_distr.unstack(level=1).plot(kind='bar', subplots=True, rot=0, figsize=(16,10), layout=(1,3), xlabel = \"Status\", ylabel = \"Count\", title = \"Individual Cores Distribution\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef94b1e",
   "metadata": {},
   "source": [
    "## Runtime Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f54406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Runtime statistics\n",
    "runtime_mean = all_jobs.groupby(['status','cores']).mean()\n",
    "runtime_mean = runtime_mean[\"runtime\"]\n",
    "runtime_max = all_jobs.groupby(['status','cores']).max()\n",
    "runtime_max = runtime_max[\"runtime\"]\n",
    "runtime_min = all_jobs.groupby(['status','cores']).min()\n",
    "runtime_min = runtime_min[\"runtime\"]\n",
    "runtime_median = all_jobs.groupby(['status','cores']).median()\n",
    "runtime_median = runtime_median[\"runtime\"]\n",
    "runtime_std = all_jobs.groupby(['status','cores']).std()\n",
    "runtime_std = runtime_std[\"runtime\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b6b31",
   "metadata": {},
   "source": [
    "## Mean Runtime of Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean Runtime of Jobs per Core\\n {runtime_mean}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f163302",
   "metadata": {},
   "source": [
    "## Max Runtime of Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7bac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Max Runtime of Jobs per Core\\n {runtime_max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa20fcc",
   "metadata": {},
   "source": [
    "## Minimum Runtime of Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484217dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Min Runtime of Jobs per Core\\n {runtime_min}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122a9d5",
   "metadata": {},
   "source": [
    "## Median Runtime of Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe8a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Median Runtime of Jobs per Core\\n {runtime_median}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57265b7d",
   "metadata": {},
   "source": [
    "## Runtime Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0edc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Runtime Standard Deviation per Core\\n {runtime_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ecd7eb",
   "metadata": {},
   "source": [
    "## Histogram of Job Runtimes per core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8133d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Success_filter = all_jobs[\"status\"] == \"succeeded\" #Only TSP Jobs\n",
    "filtered_data = all_jobs.where(Success_filter)\n",
    "filtered_data = filtered_data.dropna()\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1fb20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_filter_5 =  filtered_data[\"cores\"] == 5.0 #Only TSP Jobs\n",
    "core_data_5 = filtered_data.where(core_filter_5)\n",
    "core_data_5 = core_data_5.dropna()\n",
    "core_data_5\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.hist(core_data_5[\"runtime\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bfecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_filter_4 =  filtered_data[\"cores\"] == 4.0 #Only TSP Jobs\n",
    "core_data_4 = filtered_data.where(core_filter_4)\n",
    "core_data_4 = core_data_4.dropna()\n",
    "core_data_4\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.hist(core_data_4[\"runtime\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_filter_2 =  filtered_data[\"cores\"] == 2.0 #Only TSP Jobs\n",
    "core_data_2 = filtered_data.where(core_filter_2)\n",
    "core_data_2 = core_data_2.dropna()\n",
    "core_data_2\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.hist(core_data_2[\"runtime\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f05ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_filter_2 =  filtered_data[\"cores\"] == 2.0 #Only TSP Jobs\n",
    "core_data_2 = filtered_data.where(core_filter_2)\n",
    "core_data_2 = core_data_2.dropna()\n",
    "core_data_2\n",
    "\n",
    "sns.set(rc={'figure.figsize':(15,7)})\n",
    "sns.kdeplot(core_data_2[\"runtime\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7732da9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
